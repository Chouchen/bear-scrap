diff --git a/README.md b/README.md
index d35f72b..acf4d8b 100644
--- a/README.md
+++ b/README.md
@@ -1,46 +1,147 @@
 # Bear-Scrap
+
+## Summary
+
+## Live demo
+
+Self-hosted page : [bearscrap.nanuq.me](https://bearscrap.nanuq.me)
+
+## Screenshot
+
+![Screenshot](docs/screenshot.png)
+
 ## Presentation
-Bearstech, a company specializing in hosting and IT management, publishes a post on LinkedIn every day throughout the summer to showcase free software. The initiative is really cool, but it's not always easy to find these posts on LinkedIn and keep track of them. Being a free software enthusiast, transitioning into IT, and having "OursBlanc" as my nickname, I had to do something about it!
+
+Bearstech, a company specializing in hosting and IT management, publishes a post on LinkedIn every day throughout the summer to showcase free software. The initiative is really cool, but it's not always easy to find these posts on LinkedIn and keep track of them. Being a free software enthusiast, transitioning into IT, and having "OursBlanc" (mean Polar Bear) as my nickname, I had to do something about it!
 
 ## Idea
-Create a scraping system to retrieve each software presented in these posts and display them statically on a web page to keep a record and find them easily.
+
+Create a scraping system to retrieve each software presented in these posts and display them on a beautiful, responsive web page to keep a record and find them easily.
 
 I have just completed a year of retraining in IT, so please forgive any minor deviations from best practices as I may not yet have all the instincts and still have much to learn.
 
 ## How it works?
-I wanted to use an API to scrape posts from the Bearstech page, but it seems it's not possible to scrape company pages that way. So, I used Selenium to scrape the page "like a human."
+
+I wanted to use an API to scrape posts from the Bearstech page, but it seems it's not possible to scrape company pages that way. So, I used **Selenium** to scrape the page "like a human."
 
 The operation is quite simple:
-There is a CSV file that records the posts from the Bearstech page. (Given that there normally won't be a huge number of entries, I preferred using a .csv over a real database.)
 
-We log in to LinkedIn using the credentials contained in the config file, then we go to the Bearstech page, scroll through the posts, and when we come across a post titled "Les logiciels libres de l'été," we expand the post, scrape it using a regex, and set it aside (unless that post is already in the .csv). Once we reach the "Logiciel de l'été jour 1" (the first one), we stop there and save everything in the .csv.
+- There is a **CSV file** (`data/list.csv`) that records the posts from the Bearstech page
+- Given that there normally won't be a huge number of entries, I preferred using a `.csv` over a real database
+
+### Project Structure
+
+```
+bear-scrap/
+├── backend/              # Python scripts
+│   ├── main.py          # Main scraping script
+│   ├── populate_csv.py  # CSV population functions
+│   ├── read_csv.py      # CSV reading functions
+│   └── server.py        # Local development server
+├── data/                # Data files
+│   ├── list.csv        # Scraped projects data
+│   └── config          # LinkedIn credentials
+├── web/                 # Frontend files
+│   ├── index.html      # Main page
+│   ├── script.js       # JavaScript functionality
+│   ├── style.css       # Styling
+│   └── assets/         # Images and resources
+├── docs/                # Documentation
+│   └── screenshot.png  # Project screenshot
+└── requirements.txt     # Python dependencies
+```
+
+### Process flow:
+
+1. **Login**: We log in to LinkedIn using the credentials contained in the data/config file
+2. **Navigate**: Go to the Bearstech page and scroll through the posts
+3. **Identify**: When we find a post titled _"Les logiciels libres de l'été"_, we expand it
+4. **Extract**: Scrape the post content using a regex pattern
+5. **Filter**: Skip posts that are already in the `.csv` file
+6. **Stop condition**: Once we reach _"Logiciel de l'été jour 1"_ (the first one), we stop
+7. **Save**: Store all new entries in the `.csv` file
+
+Then everything is shown in a nice website with JS to make DOM
 
 ## Installation
-Download the repository
-setup the env
-launch the script
-wait approx 2 minutes
 
-then launch the index.html and "voila" you have a webpage with nice formating
+### Requirements
+
+- Python 3.x
+- Firefox browser
+- LinkedIn account credentials
+
+### Populate the CSV
+
+1. **Download** the repository
+
+   ```bash
+   git clone https://github.com/OursBlanc42/bear-scrap.git
+   cd bear-scrap
+   ```
+
+2. **Set up** the environment
 
-### Deployment
-I've self hosted the project here, with a cron task, i schedule each day at 1:00 the scan of linkedin to continuously populate the page
+   ```bash
+   pip install -r requirements.txt
+   ```
 
+3. **Edit** data/config file with your credentials
+
+   ```bash
+   nano data/config
+   # Add your LinkedIn credentials:
+   # [credential]
+   # email = your-email@example.com
+   # password = your-password
+   ```
+
+4. **Launch** the script
+
+   ```bash
+   python backend/main.py
+   ```
+
+5. **Wait** a few minutes
+
+After that, the script has populated the `.csv` and you have your list in the `.csv` file!
+
+### Local testing
+
+You can run a simple server for local testing to see the website with the CSV formatting:
+
+```bash
+python backend/server.py
+```
+
+Then open your browser and go to [http://localhost:8000/web/](http://localhost:8000/web/) to view the site locally.
+
+## Deployment & Infrastructure
+
+The project is self-hosted on my **Helios64** server. Using **Caddy** as a web server.
+
+- **Automated updates**: A cron job runs daily at 1:00 AM to scan LinkedIn and keep the page continuously updated with new software releases
+- **Live demo**: [bearscrap.nanuq.me](https://bearscrap.nanuq.me)
 
 ## Disclaimer
-Apparently, linkedin n'aime pas trop qu'on fasse plein de requete comme ça, donc il faut utiliser ça avec parcimonie. Vu que l'on se log avec un compte utilisateur je pense qu'il y a un risque de se faire perma-ban.
 
+Apparently, LinkedIn doesn't really like it when you make a lot of requests like this, so it should be used sparingly. Since we log in with a user account, I think there's a risk of getting permanently banned.
 
+## Improvements
 
-## Improvement
-La logique algorithmique peut certainement être amélioré (j'ai essayé d'optimiser ça mais j'ai pas réussi à trouver un moyen de limiter le nombre de boucle avec Selenium qui rescan tous les posts à chaque fois... Dans l'idéal il faudrait scanner que les nouveaux poste une fois qu'on a scrollé et afficher de nouvelles publications
+- The algorithmic logic could certainly be improved (I tried to optimize it but couldn't find a way to limit the number of loops with Selenium that rescans all posts each time). Ideally, it would only scan new posts once we've scrolled and display new publications.
+- Minor improvements
+- Refactorize code
+- Optimize execution time of main.py
 
 ## License
+
 [![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)
 
 ## Made with love
+
 I made this myself with my bear paws. It's a small personal project to populate my GitHub and help in my job search.
 
-## Remerciements
-https://codepen.io/uchardon/pen/bxbqoG/ pour le footer "vague" que j'ai repimpé à ma sauce derriere
+## Acknowledgments
 
+Thanks to [uchardon](https://codepen.io/uchardon/pen/bxbqoG/) for the original wave footer animation that I customized and enhanced for this project.
diff --git a/config b/config
deleted file mode 100644
index 3d4a314..0000000
--- a/config
+++ /dev/null
@@ -1,4 +0,0 @@
-[credential]
-email = regniersimon@hotmail.com
-password = Jesuislié42!
-
diff --git a/list.csv b/list.csv
deleted file mode 100644
index 0864780..0000000
--- a/list.csv
+++ /dev/null
@@ -1,5 +0,0 @@
-23,Coolify,une alternative Open Source à Heroku/Netlify,https://lnkd.in/dxDmcEwM,https://coolify.io/,https://lnkd.in/d8XNEGfg
-22,LanguageTool,un correcteur d’orthographe et de grammaire multilingue Open Source.,https://www.linkedin.com/company/languagetool/,https://lnkd.in/dbm-JZat,https://lnkd.in/dyNyw7G2
-21,Syncthing,un programme Open Source de synchronisation continue de fichiers. Il permet de synchroniser des fichiers entre plusieurs ordinateurs sans serveur centralisé. Conçu pour être facile à utiliser et multi-plateforme.,https://lnkd.in/dfYSMNHv,https://syncthing.net/,https://lnkd.in/dzzdeTM5
-20,Metabase,une alternative Open Source à Tableau,https://www.linkedin.com/company/metabase/,https://lnkd.in/gj6Q9nxG,https://www.metabase.com/
-19,drawDB,"Un outil de conception de bases de données Open Source et générateur SQL. Créez des diagrammes et exportez le script DDL à exécuter sur votre base de données, ou optez pour le format JSON ou une image.",https://www.linkedin.com/company/drawdb/,https://lnkd.in/eTjCyj6X,https://www.drawdb.app/
diff --git a/main.py b/main.py
deleted file mode 100644
index b12cd19..0000000
--- a/main.py
+++ /dev/null
@@ -1,229 +0,0 @@
-#!/usr/bin/env python3
-
-"""
-Main script to scrape LinkedIn posts from the BearsTech page.
-
-This script logs into LinkedIn, navigates to the BearsTech page,
-and scans for posts related to "Les Logiciels Libres de l'été".
-It saves the found posts to a CSV file and avoids duplicates
-by checking against previously saved posts.
-"""
-
-# Import libraries
-from selenium import webdriver
-from selenium.webdriver.firefox.options import Options
-from selenium.webdriver.common.by import By
-from selenium.webdriver.support.ui import WebDriverWait
-from selenium.webdriver.support import expected_conditions as EC
-import configparser
-import re
-import time
-import traceback
-from populate_csv import save_found_posts
-from read_csv import read_csv
-
-# Execution time calculation
-start_time = time.time()
-
-# Load configuration from config file
-config = configparser.ConfigParser()
-config.read('./config')
-email = config.get('credential', 'email')
-password = config.get('credential', 'password')
-
-options = Options()
-options.add_argument("--headless")
-options.add_argument("--disable-gpu")
-driver = webdriver.Firefox(options=options)
-
-# Read known posts from CSV
-known_posts = read_csv()
-
-try:
-    # Go to LinkedIn login page
-    print("🌐\tNavigating to LinkedIn login page...")
-    driver.get("https://www.linkedin.com/login")
-
-    # Wait for the login form to load
-    wait = WebDriverWait(driver, 10)
-
-    # Find and fill email field
-    print("✉️\tFilling the email field...")
-    email_field = wait.until(
-        EC.presence_of_element_located((By.ID, "username"))
-    )
-    email_field.send_keys(email)
-
-    # Find and fill password field
-    print("🔑\tFilling the password field...")
-    password_field = driver.find_element(By.ID, "password")
-    password_field.send_keys(password)
-
-    # Find and click login button
-    print("🔓\tClicking the login button...")
-    login_button = driver.find_element(By.XPATH, "//button[@type='submit']")
-    login_button.click()
-
-    # Wait for login to complete (wait for redirect or main page element)
-    print("⏳\tWaiting for login to complete...")
-    wait.until(
-        EC.url_contains("linkedin.com/feed") or
-        EC.presence_of_element_located((By.TAG_NAME, "main"))
-    )
-    print("✅\tLogin successful!")
-
-    # Now navigate to the desired page
-    print("🐻\tNavigating to BearsTech page...")
-    bearstech_url = "https://www.linkedin.com/company/bearstech/posts/"
-    driver.get(f"{bearstech_url}?feedView=all")
-
-    # Wait for the page content to load 
-    wait.until(EC.presence_of_element_located((By.TAG_NAME, "main")))
-    print("🌐\tSuccessfully arrived on the BearsTech page!")
-
-    # Scroll and scan posts until we find "Les logiciels libre de l'été 1"
-    print("🔍\tScanning posts while scrolling...")
-
-    # Initialize variables for found posts and target post
-    found_posts = []
-    target_post_found = False
-    scroll_count = 0
-    max_scrolls = 20
-    expanded_count = 0
-
-    while not target_post_found and scroll_count < max_scrolls:
-        print(f"🔽\t Scroll n° {scroll_count + 1}...")
-
-        # Unroll each post to ensure we can read the full content
-        # ('... plus' button (in French))
-        print("↕️\tExpanding all posts...")
-        see_more_spans = driver.find_elements(
-            By.XPATH, "//span[contains(text(), '… plus')]"
-        )
-
-        for span in see_more_spans:
-            try:
-                # Scroll the span into view
-                driver.execute_script(
-                    "arguments[0].scrollIntoView({block: 'center'});", span
-                )
-
-                time.sleep(5)
-
-                span.click()
-                expanded_count += 1
-
-                time.sleep(5)
-
-            except Exception:
-                continue
-
-        print(f"🔢\tExpanded {expanded_count} posts")
-
-        # Wait for all content to load after expansion
-        time.sleep(2)
-
-        # Use the selector that targets the complete post content
-        # This selector captures the full expanded text with links
-        posts = driver.find_elements(
-            By.CSS_SELECTOR, "span.break-words.tvm-parent-container"
-        )
-
-        print(f"📄\tFound {len(posts)} posts to analyze")
-
-        new_posts_count = 0
-
-        # Analyze each post
-        for post in posts:
-            try:
-                post_text = post.text
-
-                # Check if it's the target post (day 1)
-                if "Les Logiciels Libres de l'été, jour 1 :" in post_text:
-                    print("🎯\tFound target post: " +
-                          "'Les Logiciels Libres de l'été, jour 1 :'")
-                    target_post_found = True
-
-                # Apply regex for the posts we're interested in
-                main_regex = re.search(
-                    r"jour\s*(\d{1,2})\s*[:\-–]?\s*\n*([^:]+)\s*:\s*(.+?)"
-                    r"(?:\n|$)",
-                    post_text,
-                    re.IGNORECASE | re.DOTALL
-                )
-                
-                # If regex matches, extract the day, title, and description
-                if main_regex:
-                    day = main_regex.group(1)
-                    title = main_regex.group(2).strip()
-                    description = main_regex.group(3).strip()
-
-                    # Check if we already know this post (by day)
-                    # Then add it if it's new or ignore it if known
-                    if day not in known_posts:
-                        known_posts.add(day)
-                        new_posts_count += 1
-
-                        # Extract all links from the post
-                        links = []
-                        link_elements = post.find_elements(By.TAG_NAME, "a")
-                        for link_el in link_elements:
-                            href = link_el.get_attribute('href')
-                            if href:
-                                links.append(href)
-
-                        found_posts.append({
-                            'text': post_text,
-                            'day': day,
-                            'title': title,
-                            'description': description,
-                            'links_project': links[0] if len(links) > 0 else None,
-                            'links_more': links[1] if len(links) > 1 else None,
-                            'links_support': links[2] if len(links) > 2 else None
-                        })
-                        print(f"⏺️\tFound NEW post: jour {day} - {title}")
-                    else:
-                        print(f"⏭️\tSkipped known post: jour {day}")
-
-            except Exception as e:
-                print(f"⚠️\tError processing post: {e}")
-                continue
-
-        print(f"🔎\tFound {new_posts_count} new posts in this scroll")
-
-        if not target_post_found:
-            # Scroller vers le bas pour charger plus de contenu
-            print("🔽\tScrolling down to load more posts...")
-            driver.execute_script(
-                "window.scrollTo(0, document.body.scrollHeight);"
-            )
-            time.sleep(5)
-            scroll_count += 1
-
-    if target_post_found:
-        print("🏆\tSuccessfully reached target post after " +
-              f"{scroll_count} scrolls")
-    else:
-        print(f"❌\tTarget post not found after {max_scrolls} scrolls")
-
-    # Save all found posts
-
-    print(f"\n📋\tSUMMARY: Found {len(found_posts)} matching posts:")
-    if len(found_posts) > 0:
-        print("💾\tSaving found posts to CSV file...")
-        save_found_posts(found_posts)
-    else:
-        print("❌\tNo posts to save")
-
-
-except Exception as e:
-    print(f"⚠️\tError during execution: {e}")
-    traceback.print_exc()
-
-finally:
-    # Close the browser
-    driver.quit()
-
-    # Calculate and print execution time
-    execution_time = time.time() - start_time
-    print(f"⏱️\tExecution time: {execution_time:.2f} seconds")
diff --git a/populate_csv.py b/populate_csv.py
deleted file mode 100644
index 3c50228..0000000
--- a/populate_csv.py
+++ /dev/null
@@ -1,25 +0,0 @@
-#!/usr/bin/env python3
-
-"""
-This function appends new posts to the existing CSV file.
-It assumes each post is a dictionary with keys 'day', 'title',
-'links_project', 'links_more', and 'links_support'.
-"""
-
-import csv
-
-
-def save_found_posts(posts):
-    # Save found posts to a CSV file
-
-    with open("list.csv", "a") as f:
-        writer = csv.writer(f)
-        for post in posts:
-            writer.writerow(
-                [post['day'],
-                 post['title'],
-                 post['description'],
-                 post.get('links_project', ''),
-                 post.get('links_more', ''),
-                 post.get('links_support', '')])
-    print("✅\tPosts saved successfully!")
diff --git a/read_csv.py b/read_csv.py
deleted file mode 100644
index 62a2c17..0000000
--- a/read_csv.py
+++ /dev/null
@@ -1,29 +0,0 @@
-#!/usr/bin/env python3
-
-"""
-Read a CSV file and return a set of known posts (based on the 'day' field).
-
-Returns:
-    set: A set of known post based on the 'day' field.
-"""
-
-import csv
-
-
-def read_csv():
-    # Read found posts from a CSV file and return a set of known days
-    known_posts = set()
-
-    # Try to read the CSV file then check if it exists
-    # If it does, read the 'day' field from each row and add it to the set
-    try:
-        with open("list.csv", "r") as f:
-            reader = csv.reader(f)
-            for row in reader:
-                if len(row) >= 1:
-                    known_posts.add(row[0])
-    except FileNotFoundError:
-        print("📄\tNo existing CSV file found, starting fresh...")
-
-    print(f"✅\t{len(known_posts)} known posts loaded")
-    return known_posts
diff --git a/web/index.html b/web/index.html
index ede5b5f..4eff6e5 100644
--- a/web/index.html
+++ b/web/index.html
@@ -7,6 +7,7 @@
     <link rel="stylesheet" href="style.css">
 </head>
 <body>
+
     <header>
         <div>
             <h1>🏖️ Les Logiciels Libres de l'été</h1>
@@ -32,7 +33,7 @@
                             </tr>
                         </thead>
                         <tbody id="projectsGrid">
-                            <!-- Les projets seront chargés dynamiquement ici -->
+                            <!-- project will be loaded dynamically here -->
                         </tbody>
                     </table>
                 </div>
@@ -53,17 +54,21 @@
             </section>
         </div>
     </main>
-    <footer>
-        <svg viewBox="0 0 120 16">
+    
+    <footer id="footer-back">
+        <svg class="background-waves" viewBox="0 0 120 16">
             <defs>
-                <path id="wave" d="M 0,10 C 30,10 30,15 60,15 90,15 90,10 120,10 150,10 150,15 180,15 210,15 210,10 240,10 v 28 h -240 z" />
+                <path id="wave-path" d="M 0,10 C 30,10 30,15 60,15 90,15 90,10 120,10 150,10 150,15 180,15 210,15 210,10 240,10 v 28 h -240 z" />
             </defs>
-
-            <use id="wave3" class="wave" xlink:href="#wave" x="0" y="-2" ></use> 
-            <use id="wave2" class="wave" xlink:href="#wave" x="0" y="0" ></use>
-            <use id="wave1" class="wave" xlink:href="#wave" x="0" y="1" />
+            <use id="wave3" class="wave" xlink:href="#wave-path" x="0" y="-2" ></use> 
+            <use id="wave2" class="wave" xlink:href="#wave-path" x="0" y="0" ></use>
+        </svg>
+    </footer>
+    <footer id="footer-front">
+        <svg class="foreground-wave" viewBox="0 0 120 16">
+            <use id="wave0" class="wave" xlink:href="#wave-path" x="0" y="1" />
+            <use id="wave1" class="wave" xlink:href="#wave-path" x="0" y="3" ></use>
         </svg>
-
         <div class="footer-content">
             <p>Made with ❤️ by <a href="https://github.com/OursBlanc42">OursBlanc</a></p>
         </div>
diff --git a/web/script.js b/web/script.js
index 82f5da0..3efb36b 100644
--- a/web/script.js
+++ b/web/script.js
@@ -5,7 +5,7 @@ async function loadProjects() {
 
   try {
     // Charger les données du CSV
-    const response = await fetch("../list.csv");
+    const response = await fetch("../data/list.csv");
     if (!response.ok) {
       throw new Error(`HTTP error! status: ${response.status}`);
     }
diff --git a/web/style.css b/web/style.css
index 92d8b38..320d6e2 100644
--- a/web/style.css
+++ b/web/style.css
@@ -1,15 +1,22 @@
+/* ==========================================================================
+   VARIABLES CSS
+   ========================================================================== */
 :root {
     --primary-color: #ff6b6b;
     --secondary-color: rgb(74, 188, 194);
     --background-color: #fff8e1;
     --text-color: #333;
     --card-background-color: #f9f9f9;
-    --card-border-color: #ddd;
     --light-color: #fff;
+    --border-color: #c1c1c1;
 }
 
+/* ==========================================================================
+   BASE STYLES
+   ========================================================================== */
 body {
     font-family: 'Arial', sans-serif;
+    font-size: 12px;
     margin: 0;
     padding: 0;
     background-color: var(--background-color);
@@ -17,22 +24,21 @@ body {
     overflow-x: hidden;
 }
 
+/* ==========================================================================
+   HEADER
+   ========================================================================== */
 header {
     background-color: var(--secondary-color);
-    padding: 2rem 0 0 0;
+    padding: 2rem 0;
     text-align: center;
-    position: relative;
     box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);
     border-radius: 0 0 1000px 1000px / 0 0 25px 25px;
 }
 
-
 header div {
     max-width: 800px;
     margin: 0 auto;
     padding-bottom: 2rem;
-    position: relative;
-    z-index: 3;
 }
 
 header h1 {
@@ -43,10 +49,13 @@ header h1 {
 
 header p {
     margin: 0.5rem 0 0 0;
-      color: var(--light-color);
+    color: var(--light-color);
     font-size: 1.2rem;
 }
 
+/* ==========================================================================
+   MAIN CONTENT
+   ========================================================================== */
 main {
     padding: 2rem 0;
     max-width: 800px;
@@ -54,7 +63,7 @@ main {
     min-height: 60vh;
     padding-bottom: 5rem;
     position: relative;
-    z-index: 10;
+    z-index: 2;
 }
 
 section {
@@ -67,6 +76,9 @@ section h2 {
     padding-bottom: 0.5rem;
 }
 
+/* ==========================================================================
+   LINKS
+   ========================================================================== */
 a {
     color: var(--primary-color);
     text-decoration: none;
@@ -77,6 +89,9 @@ a:hover {
     text-decoration: underline;
 }
 
+/* ==========================================================================
+   TABLE STYLES
+   ========================================================================== */
 .table-container {
     overflow-x: auto;
     margin-bottom: 2rem;
@@ -85,7 +100,7 @@ a:hover {
 .projects-table {
     width: 100%;
     border-collapse: collapse;
-    background-color: var(--light-color);
+    background-color: var(--card-background-color);
     border-radius: 8px;
     overflow: hidden;
 }
@@ -100,7 +115,7 @@ a:hover {
 
 .projects-table td {
     padding: 1rem;
-    border-bottom: 1px solid var(--card-border-color);
+    border-bottom: 1px solid var(--border-color);
     vertical-align: top;
 }
 
@@ -119,20 +134,23 @@ a:hover {
     line-height: 1.4;
 }
 
-/* Utilisation du même style que les liens du footer */
+/* ==========================================================================
+   BUTTON STYLES (liens dans le tableau)
+   ========================================================================== */
 .links-cell a {
     display: inline-block;
     margin-right: 0.5rem;
     margin-bottom: 0.5rem;
-    padding: 0.5rem 1rem;
+    padding: 0.3rem 0.6rem;
     color: var(--light-color);
     text-decoration: none;
     border-radius: 25px;
     background-color: var(--secondary-color);
-    font-size: 0.9rem;
-    font-weight: 300;
     transition: 0.3s ease;
+    font-weight: 300;
+    font-size: 0.75rem;
     opacity: 0.9;
+    white-space: nowrap;
 }
 
 .links-cell a:hover {
@@ -141,7 +159,9 @@ a:hover {
     transform: translateY(-2px);
 }
 
-
+/* ==========================================================================
+   STATISTICS CARDS
+   ========================================================================== */
 .stats-grid {
     display: grid;
     grid-template-columns: repeat(2, 1fr);
@@ -152,10 +172,9 @@ a:hover {
 
 .stat-card {
     background-color: var(--card-background-color);
-    border: 1px solid var(--card-border-color);
+    border: 1px solid var(--border-color);
     border-radius: 8px;
     padding: 1rem;
-    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.2);
     text-align: center;
 }
 
@@ -164,22 +183,49 @@ a:hover {
     color: var(--secondary-color);
 }
 
-/* Footer avec SVG animé */
-footer {
-    width: 100vw;
+/* ==========================================================================
+   FOOTER & WAVES
+   ========================================================================== */
+
+/* Footer avec vagues arrière-plan */
+#footer-back {
     position: fixed;
-    bottom: 0px;
-    margin-left: calc(50% - 50vw);
+    left: 0;
+    right: 0;
+    bottom: 40px;
     z-index: 1;
 }
 
+/* Footer avec vague avant et contenu */
+#footer-front {
+    position: fixed;
+    left: 0;
+    right: 0;
+    bottom: 0;
+    z-index: 3;
+    pointer-events: none;
+}
+
 footer svg {
     width: 100%;
     height: auto;
     pointer-events: none;
+    display: block;
+    margin: 0;
 }
 
-footer a {
+.footer-content {
+    background-color: var(--secondary-color);
+    padding: 10px;
+    font-size: 0.8rem;
+    font-weight: bold;
+    color: var(--light-color);
+    text-align: center;
+    pointer-events: auto;
+    margin: 0;
+}
+
+.footer-content a {
     color: var(--primary-color);
     background-color: var(--light-color);
     padding: 0.2rem 0.4rem;
@@ -189,68 +235,15 @@ footer a {
     transition: 0.4s ease;
 }
 
-footer a:hover {
+.footer-content a:hover {
     background-color: var(--primary-color);
     color: var(--light-color);
     transform: translateY(-4px);
 }
 
-.footer-content {
-    background-color: var(--secondary-color);
-    margin: -5px 0px 0px 0px;
-    padding: 10px;
-    color: var(--light-color);
-    text-align: center;
-}
-
-.footer-content p {
-    margin: 10px;
-    font-size: 0.8rem;
-    font-weight: bold;
-}
-
-.menu {
-    display: flex;
-    justify-content: center;
-    align-items: center;
-    flex-wrap: wrap;
-    list-style: none;
-    padding: 0;
-    margin: 0;
-}
-
-.menu__item {
-    margin: 0 10px;
-}
-
-.menu__link {
-    font-size: 0.9rem;
-    color: var(--light-color);
-    display: inline-flex;
-    align-items: center;
-    gap: 0.5rem;
-    transition: 0.3s ease;
-    text-decoration: none;
-    opacity: 0.9;
-    font-weight: 300;
-    padding: 0.5rem 1rem;
-    border-radius: 25px;
-    background: rgba(0, 0, 0, 0.2);
-    position: relative;
-}
-
-.menu__link:hover {
-    opacity: 1;
-    background: rgba(255, 255, 255, 0.2);
-    transform: translateY(-2px);
-}
-
-.menu__link svg {
-    width: 14px;
-    height: 14px;
-    z-index: 2;
-}
-
+/* ==========================================================================
+   ANIMATIONS
+   ========================================================================== */
 @keyframes wave {
     to {
         transform: translateX(-100%);
@@ -258,33 +251,36 @@ footer a:hover {
 }
 
 .wave {
-    animation: wave 3s linear;
-    animation-iteration-count: infinite;
-    fill: rgba(74, 188, 194, 1);
+    animation: wave 4s linear infinite;
+    fill: var(--secondary-color);
 }
 
+#wave0 {
+    fill: var(--secondary-color);
+}
 
 #wave1 {
-    fill: rgba(74, 188, 194, 1);
+    animation-duration: 5s;
+    animation-direction: reverse;
+    opacity: 0.5;
 }
 
 #wave2 {
-    animation-duration: 5s;
+    animation-duration: 7s;
     animation-direction: reverse;
     opacity: 0.6;
-    fill: rgba(74, 188, 194, 0.7);
 }
 
 #wave3 {
-    animation-duration: 7s;
+    animation-duration: 9s;
     opacity: 0.3;
-    fill: rgba(74, 188, 194, 0.6);
 }
 
-
-
-/* Responsive pour mobile */
+/* ==========================================================================
+   RESPONSIVE DESIGN
+   ========================================================================== */
 @media (max-width: 768px) {
+    /* Header responsive */
     header h1 {
         font-size: 2rem;
     }
@@ -293,6 +289,13 @@ footer a:hover {
         font-size: 1rem;
     }
     
+    /* Main content responsive */
+    main {
+        padding: 1rem 0;
+        padding-bottom: 6rem;
+    }
+    
+    /* Table responsive */
     .projects-table {
         font-size: 0.9rem;
     }
@@ -302,8 +305,47 @@ footer a:hover {
         padding: 0.7rem;
     }
     
+    /* Button responsive */
+    .links-cell a {
+        padding: 0.25rem 0.5rem;
+        font-size: 0.7rem;
+        margin-bottom: 0.3rem;
+    }
+    
+    /* Stats cards responsive */
+    .stats-grid {
+        grid-template-columns: 1fr;
+        gap: 0.5rem;
+        margin-bottom: 100px;
+    }
+    
+    .stat-card {
+        padding: 0.8rem;
+    }
+}
+
+@media (max-width: 480px) {
+    /* Very small screens */
+    body {
+        font-size: 11px;
+    }
+    
+    header h1 {
+        font-size: 1.8rem;
+    }
+    
+    main {
+        padding: 0.5rem;
+        padding-bottom: 7rem;
+    }
+    
+    .projects-table th,
+    .projects-table td {
+        padding: 0.5rem;
+    }
+    
     .links-cell a {
-        padding: 0.4rem 0.8rem;
-        font-size: 0.8rem;
+        padding: 0.2rem 0.4rem;
+        font-size: 0.65rem;
     }
 }
\ No newline at end of file
